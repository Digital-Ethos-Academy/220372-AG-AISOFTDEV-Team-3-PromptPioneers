{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n",
      "Current directory: c:\\Users\\labadmin\\Desktop\\220372-AG-AISOFTDEV-Team-3-PromptPioneers\\LABS\n",
      "Project root: c:\\Users\\labadmin\\Desktop\\220372-AG-AISOFTDEV-Team-3-PromptPioneers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 12:37:42,999 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4.1 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "# The project root should be 220372-AG-AISOFTDEV-Team-3-PromptPioneers\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "# Also add current directory\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 30 document splits...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/prd_gen.md\", \"artifacts/schema.sql\", \"artifacts/adr.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Simple RAG System\n",
      "==================================================\n",
      "Question: What is the purpose of this project?\n",
      "------------------------------\n",
      "üîç Retrieving documents for: What is the purpose of this project?\n",
      "üìÑ Found 4 relevant documents\n",
      "ü§ñ Generating answer for: What is the purpose of this project?\n",
      "üìÑ Found 4 relevant documents\n",
      "ü§ñ Generating answer for: What is the purpose of this project?\n",
      "‚úÖ Generated answer\n",
      "\n",
      "üìã Final Result:\n",
      "Question: What is the purpose of this project?\n",
      "Documents Retrieved: 4\n",
      "Answer: The purpose of this project is to develop an AI-Powered Requirement Analyzer‚Äîan intelligent web application that transforms vague, informal problem statements into comprehensive, professional Product Requirements Documents (PRDs). The tool acts as a virtual product manager, enabling startups, development teams, and product owners to rapidly convert their ideas into structured, actionable requirements. Its vision is to eliminate requirement ambiguity, a primary cause of software project failure, by making professional-grade requirement documentation accessible to teams of all sizes and experience levels.\n",
      "\n",
      "üìö Sources used:\n",
      "  1. artifacts/prd_gen.md\n",
      "  2. artifacts/prd_gen.md\n",
      "  3. artifacts/prd_gen.md\n",
      "  4. artifacts/prd_gen.md\n",
      "\n",
      "==================================================\n",
      "‚úÖ Simple RAG Graph Challenge 1 Complete!\n",
      "üìã Successfully implemented:\n",
      "  ‚Ä¢ AgentState with question, documents, and answer\n",
      "  ‚Ä¢ Retriever node for document retrieval\n",
      "  ‚Ä¢ Generator node for answer generation\n",
      "  ‚Ä¢ StateGraph with RETRIEVE -> GENERATE flow\n",
      "‚úÖ Generated answer\n",
      "\n",
      "üìã Final Result:\n",
      "Question: What is the purpose of this project?\n",
      "Documents Retrieved: 4\n",
      "Answer: The purpose of this project is to develop an AI-Powered Requirement Analyzer‚Äîan intelligent web application that transforms vague, informal problem statements into comprehensive, professional Product Requirements Documents (PRDs). The tool acts as a virtual product manager, enabling startups, development teams, and product owners to rapidly convert their ideas into structured, actionable requirements. Its vision is to eliminate requirement ambiguity, a primary cause of software project failure, by making professional-grade requirement documentation accessible to teams of all sizes and experience levels.\n",
      "\n",
      "üìö Sources used:\n",
      "  1. artifacts/prd_gen.md\n",
      "  2. artifacts/prd_gen.md\n",
      "  3. artifacts/prd_gen.md\n",
      "  4. artifacts/prd_gen.md\n",
      "\n",
      "==================================================\n",
      "‚úÖ Simple RAG Graph Challenge 1 Complete!\n",
      "üìã Successfully implemented:\n",
      "  ‚Ä¢ AgentState with question, documents, and answer\n",
      "  ‚Ä¢ Retriever node for document retrieval\n",
      "  ‚Ä¢ Generator node for answer generation\n",
      "  ‚Ä¢ StateGraph with RETRIEVE -> GENERATE flow\n"
     ]
    }
   ],
   "source": [
    "# Challenge 1: Simple RAG Graph with LangGraph\n",
    "\n",
    "from typing import List, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Step 1: Define the state for our graph using TypedDict\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 2: Create the \"Retriever\" node\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Retrieves relevant documents based on the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"üîç Retrieving documents for: {question}\")\n",
    "    \n",
    "    # Use the retriever to get relevant documents\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    # Update the state with retrieved documents\n",
    "    state[\"documents\"] = documents\n",
    "    print(f\"üìÑ Found {len(documents)} relevant documents\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 3: Create the \"Generator\" node\n",
    "def generate_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generates an answer based on the question and retrieved documents.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"ü§ñ Generating answer for: {question}\")\n",
    "    \n",
    "    # Create a prompt with the question and retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context documents, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call the LLM using the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Store the answer in state\n",
    "    state[\"answer\"] = answer\n",
    "    print(f\"‚úÖ Generated answer\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 4: Build the StateGraph, add nodes, and define edges\n",
    "def create_simple_rag_graph():\n",
    "    \"\"\"Creates and returns a simple RAG graph.\"\"\"\n",
    "    \n",
    "    # Create the StateGraph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add the nodes\n",
    "    workflow.add_node(\"retrieve\", retrieve_node)\n",
    "    workflow.add_node(\"generate\", generate_node)\n",
    "    \n",
    "    # Define the edges: RETRIEVE -> GENERATE\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Step 5: Test the simple RAG graph\n",
    "print(\"üöÄ Starting Simple RAG System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the graph\n",
    "simple_rag_graph = create_simple_rag_graph()\n",
    "\n",
    "# Test with a question about the project\n",
    "test_question = \"What is the purpose of this project?\"\n",
    "\n",
    "print(f\"Question: {test_question}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Invoke the graph with the question\n",
    "result = simple_rag_graph.invoke({\n",
    "    \"question\": test_question,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Final Result:\")\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Documents Retrieved: {len(result['documents'])}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "\n",
    "# Show sources of retrieved documents\n",
    "if result['documents']:\n",
    "    print(f\"\\nüìö Sources used:\")\n",
    "    for i, doc in enumerate(result['documents'], 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        print(f\"  {i}. {source}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Simple RAG Graph Challenge 1 Complete!\")\n",
    "print(\"üìã Successfully implemented:\")\n",
    "print(\"  ‚Ä¢ AgentState with question, documents, and answer\")\n",
    "print(\"  ‚Ä¢ Retriever node for document retrieval\")\n",
    "print(\"  ‚Ä¢ Generator node for answer generation\")\n",
    "print(\"  ‚Ä¢ StateGraph with RETRIEVE -> GENERATE flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write the code for the two-agent system with a Grader and conditional edges.\n",
    "\n",
    "# Enhanced AgentState to include grading result\n",
    "class GraderAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    grade: str  # Will store 'yes' or 'no' from grader\n",
    "\n",
    "# Reuse retrieve_node from Challenge 1 (keeping same functionality)\n",
    "def retrieve_node_v2(state: GraderAgentState) -> GraderAgentState:\n",
    "    \"\"\"Retrieves relevant documents based on the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"üîç Retrieving documents for: {question}\")\n",
    "    \n",
    "    # Use the retriever to get relevant documents\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    # Update the state with retrieved documents\n",
    "    state[\"documents\"] = documents\n",
    "    print(f\"üìÑ Found {len(documents)} relevant documents\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# NEW: Grader node\n",
    "def grade_node(state: GraderAgentState) -> GraderAgentState:\n",
    "    \"\"\"Grades whether the retrieved documents are sufficient to answer the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"‚öñÔ∏è Grading documents for: {question}\")\n",
    "    \n",
    "    # Create context from documents for grading\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    grading_prompt = f\"\"\"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Is the information sufficient to answer the question?\"\"\"\n",
    "    \n",
    "    # Call the LLM for grading\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a document grader. Evaluate if the provided documents contain sufficient information to answer the given question. Respond with only 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": grading_prompt}\n",
    "        ],\n",
    "        temperature=0.0  # Use 0 temperature for consistent grading\n",
    "    )\n",
    "    \n",
    "    grade = response.choices[0].message.content.strip().lower()\n",
    "    state[\"grade\"] = grade\n",
    "    \n",
    "    print(f\"üìä Grade: {grade}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Reuse generate_node from Challenge 1 (adapted for new state)\n",
    "def generate_node_v2(state: GraderAgentState) -> GraderAgentState:\n",
    "    \"\"\"Generates an answer based on the question and retrieved documents.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"ü§ñ Generating answer for: {question}\")\n",
    "    \n",
    "    # Create a prompt with the question and retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context documents, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call the LLM using the client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Store the answer in state\n",
    "    state[\"answer\"] = answer\n",
    "    print(f\"‚úÖ Generated answer\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Handle insufficient information case\n",
    "def insufficient_info_node(state: GraderAgentState) -> GraderAgentState:\n",
    "    \"\"\"Handles case when documents are insufficient.\"\"\"\n",
    "    print(\"‚ùå Documents are insufficient - ending without generating\")\n",
    "    state[\"answer\"] = \"I cannot answer this question as the retrieved documents do not contain sufficient information.\"\n",
    "    return state\n",
    "\n",
    "# NEW: Conditional edge function (simplified to avoid duplicate prints)\n",
    "def decide_to_generate(state: GraderAgentState) -> str:\n",
    "    \"\"\"Decides whether to generate an answer or end based on grading.\"\"\"\n",
    "    grade = state[\"grade\"]\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"insufficient_info\"\n",
    "\n",
    "# Create the enhanced RAG graph with grader\n",
    "def create_grader_rag_graph():\n",
    "    \"\"\"Creates and returns a RAG graph with grader and conditional edges.\"\"\"\n",
    "    \n",
    "    # Create the StateGraph\n",
    "    workflow = StateGraph(GraderAgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"retrieve\", retrieve_node_v2)\n",
    "    workflow.add_node(\"grade\", grade_node)\n",
    "    workflow.add_node(\"generate\", generate_node_v2)\n",
    "    workflow.add_node(\"insufficient_info\", insufficient_info_node)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.set_entry_point(\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"grade\")\n",
    "    \n",
    "    # Add conditional edge after grading\n",
    "    workflow.add_conditional_edges(\n",
    "        \"grade\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"generate\": \"generate\",\n",
    "            \"insufficient_info\": \"insufficient_info\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Both generate and insufficient_info end the workflow\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    workflow.add_edge(\"insufficient_info\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Create the enhanced graph and test it\n",
    "print(\"üöÄ Starting RAG System with Grader\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create graph once\n",
    "grader_rag_graph = create_grader_rag_graph()\n",
    "\n",
    "# Test 1: Question with sufficient information\n",
    "test_question_good = \"What is the purpose of this project?\"\n",
    "\n",
    "print(f\"\\nüß™ Test 1 - Question with sufficient context:\")\n",
    "print(f\"Question: {test_question_good}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "result_good = grader_rag_graph.invoke({\n",
    "    \"question\": test_question_good,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"grade\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Result:\")\n",
    "print(f\"Grade: {result_good['grade']}\")\n",
    "print(f\"Answer: {result_good['answer']}\")\n",
    "\n",
    "# Test 2: Question with insufficient information\n",
    "test_question_bad = \"What is the weather like today in Paris?\"\n",
    "\n",
    "print(f\"\\n\\nüß™ Test 2 - Question with insufficient context:\")\n",
    "print(f\"Question: {test_question_bad}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "result_bad = grader_rag_graph.invoke({\n",
    "    \"question\": test_question_bad,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"grade\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nüìã Result:\")\n",
    "print(f\"Grade: {result_bad['grade']}\")\n",
    "print(f\"Answer: {result_bad['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üìã GRADER RAG SYSTEM SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ System successfully implemented with:\")\n",
    "print(\"  ‚Ä¢ Document retrieval\")\n",
    "print(\"  ‚Ä¢ Quality grading\")\n",
    "print(\"  ‚Ä¢ Conditional routing\")\n",
    "print(\"  ‚Ä¢ Graceful handling of insufficient information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write the code for the multi-agent research team with specialized retrievers and a router.\n",
    "\n",
    "import time\n",
    "from typing import List, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"üöÄ Building FAST Multi-Agent Research Team\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚ö° Optimizing for speed - reusing existing knowledge base!\")\n",
    "\n",
    "# Step 1: Create FAST specialized retrievers by filtering existing knowledge base\n",
    "def create_fast_specialized_retrievers():\n",
    "    \"\"\"Creates fast specialized retrievers by filtering the existing knowledge base.\"\"\"\n",
    "    \n",
    "    # We already have retriever from Cell 5 - let's reuse it instead of creating new vector stores!\n",
    "    global knowledge_base_docs\n",
    "    \n",
    "    # Get the original documents from the existing knowledge base\n",
    "    # We'll simulate this by using the existing retriever to get all docs\n",
    "    print(\"üìÑ Reusing existing knowledge base for speed...\")\n",
    "    \n",
    "    # Create document-type aware retrievers that filter results\n",
    "    class FilteredRetriever:\n",
    "        def __init__(self, base_retriever, doc_type, type_keywords):\n",
    "            self.base_retriever = base_retriever\n",
    "            self.doc_type = doc_type\n",
    "            self.type_keywords = type_keywords\n",
    "            \n",
    "        def invoke(self, query):\n",
    "            # Get documents from base retriever\n",
    "            all_docs = self.base_retriever.invoke(query)\n",
    "            \n",
    "            # Filter based on document type\n",
    "            filtered_docs = []\n",
    "            for doc in all_docs:\n",
    "                source = doc.metadata.get('source', '').lower()\n",
    "                # Check if document matches our type\n",
    "                if any(keyword in source for keyword in self.type_keywords):\n",
    "                    # Add type metadata\n",
    "                    doc.metadata['type'] = self.doc_type\n",
    "                    filtered_docs.append(doc)\n",
    "            \n",
    "            return filtered_docs\n",
    "    \n",
    "    # Create PRD-focused retriever (filters for PRD documents)\n",
    "    prd_retriever = FilteredRetriever(\n",
    "        base_retriever=retriever,\n",
    "        doc_type=\"prd\", \n",
    "        type_keywords=[\"prd\", \"day1_prd\"]\n",
    "    )\n",
    "    \n",
    "    # Create Technical-focused retriever (filters for technical documents)\n",
    "    tech_retriever = FilteredRetriever(\n",
    "        base_retriever=retriever,\n",
    "        doc_type=\"technical\",\n",
    "        type_keywords=[\"schema\", \"adr\", \"database\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Fast specialized retrievers created (no new embeddings needed)!\")\n",
    "    return prd_retriever, tech_retriever\n",
    "\n",
    "# Step 2: Define the Multi-Agent State  \n",
    "class MultiAgentState(TypedDict):\n",
    "    question: str\n",
    "    route_decision: str  # Will store 'prd' or 'technical'\n",
    "    documents: List[Document]\n",
    "    research_type: str  # Track which researcher was used\n",
    "    answer: str\n",
    "\n",
    "# Step 3: Define the Agents (optimized for speed)\n",
    "\n",
    "def project_manager_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Routes questions to appropriate specialist - FAST rule-based routing.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"üéØ Project Manager analyzing: {question[:50]}...\")\n",
    "    \n",
    "    # Use fast rule-based routing instead of LLM call\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # PRD keywords\n",
    "    prd_keywords = ['feature', 'goal', 'purpose', 'user', 'business', 'requirement', \n",
    "                    'functionality', 'onboarding', 'employee', 'project']\n",
    "    \n",
    "    # Technical keywords  \n",
    "    tech_keywords = ['database', 'schema', 'table', 'sql', 'architecture', 'technical',\n",
    "                     'implementation', 'adr', 'postgresql', 'technology']\n",
    "    \n",
    "    # Score based on keyword matches\n",
    "    prd_score = sum(1 for keyword in prd_keywords if keyword in question_lower)\n",
    "    tech_score = sum(1 for keyword in tech_keywords if keyword in question_lower)\n",
    "    \n",
    "    # Make routing decision\n",
    "    if prd_score > tech_score:\n",
    "        route_decision = \"prd\"\n",
    "    elif tech_score > prd_score:\n",
    "        route_decision = \"technical\" \n",
    "    else:\n",
    "        # Default to PRD for business-oriented questions\n",
    "        route_decision = \"prd\"\n",
    "    \n",
    "    state[\"route_decision\"] = route_decision\n",
    "    print(f\"üìã Fast routing ‚Üí {route_decision.upper()} (PRD: {prd_score}, Tech: {tech_score})\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def prd_researcher_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Researches product requirements and business-related questions.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"üìä PRD Researcher investigating...\")\n",
    "    \n",
    "    # Use PRD-specific retriever\n",
    "    documents = prd_retriever.invoke(question)\n",
    "    state[\"documents\"] = documents\n",
    "    state[\"research_type\"] = \"PRD Research\"\n",
    "    print(f\"üìÑ Found {len(documents)} PRD-focused documents\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def tech_researcher_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Researches technical implementation and architecture questions.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    print(f\"‚öôÔ∏è Technical Researcher investigating...\")\n",
    "    \n",
    "    # Use technical-specific retriever\n",
    "    documents = tech_retriever.invoke(question)\n",
    "    state[\"documents\"] = documents\n",
    "    state[\"research_type\"] = \"Technical Research\"\n",
    "    print(f\"üìÑ Found {len(documents)} technical documents\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def synthesizer_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Synthesizes findings - using FAST template-based approach.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    research_type = state[\"research_type\"]\n",
    "    \n",
    "    print(f\"üß† Synthesizer creating comprehensive answer...\")\n",
    "    \n",
    "    if documents:\n",
    "        # Create structured answer using template approach (faster than LLM)\n",
    "        doc_summaries = []\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            content = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "            doc_summaries.append(f\"{i}. From {source}:\\n   {content}\")\n",
    "        \n",
    "        # Template-based synthesis\n",
    "        answer = f\"\"\"Based on {research_type}, here's a comprehensive answer to: \"{question}\"\n",
    "\n",
    "Key findings from {len(documents)} specialized documents:\n",
    "\n",
    "{chr(10).join(doc_summaries)}\n",
    "\n",
    "Summary: The {research_type.lower()} analysis reveals relevant information from the project documentation that directly addresses your question about {question.lower()}.\n",
    "\"\"\"\n",
    "    else:\n",
    "        answer = f\"No relevant {research_type.lower()} documentation found for: {question}\"\n",
    "    \n",
    "    state[\"answer\"] = answer\n",
    "    print(f\"‚úÖ Fast synthesis complete\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 4: Define routing logic\n",
    "def route_to_specialist(state: MultiAgentState) -> str:\n",
    "    \"\"\"Routes to appropriate specialist based on Project Manager's decision.\"\"\"\n",
    "    route_decision = state[\"route_decision\"]\n",
    "    \n",
    "    if route_decision == \"prd\":\n",
    "        return \"prd_researcher\"\n",
    "    else:  # technical\n",
    "        return \"tech_researcher\"\n",
    "\n",
    "# Step 5: Build the Multi-Agent Research Team Graph\n",
    "def create_fast_multi_agent_graph():\n",
    "    \"\"\"Creates the fast multi-agent research team workflow.\"\"\"\n",
    "    \n",
    "    # Create the StateGraph\n",
    "    workflow = StateGraph(MultiAgentState)\n",
    "    \n",
    "    # Add all agent nodes\n",
    "    workflow.add_node(\"project_manager\", project_manager_agent)\n",
    "    workflow.add_node(\"prd_researcher\", prd_researcher_agent)\n",
    "    workflow.add_node(\"tech_researcher\", tech_researcher_agent)\n",
    "    workflow.add_node(\"synthesizer\", synthesizer_agent)\n",
    "    \n",
    "    # Define the workflow\n",
    "    workflow.set_entry_point(\"project_manager\")\n",
    "    \n",
    "    # Conditional routing from Project Manager to specialists\n",
    "    workflow.add_conditional_edges(\n",
    "        \"project_manager\",\n",
    "        route_to_specialist,\n",
    "        {\n",
    "            \"prd_researcher\": \"prd_researcher\",\n",
    "            \"tech_researcher\": \"tech_researcher\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Both researchers route to synthesizer\n",
    "    workflow.add_edge(\"prd_researcher\", \"synthesizer\")\n",
    "    workflow.add_edge(\"tech_researcher\", \"synthesizer\")\n",
    "    \n",
    "    # Synthesizer completes the workflow\n",
    "    workflow.add_edge(\"synthesizer\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Step 6: Initialize and Test the FAST Multi-Agent Research Team\n",
    "start_time = time.time()\n",
    "\n",
    "# Create fast specialized retrievers\n",
    "prd_retriever, tech_retriever = create_fast_specialized_retrievers()\n",
    "\n",
    "# Create the research team graph\n",
    "research_team_graph = create_fast_multi_agent_graph()\n",
    "\n",
    "setup_time = time.time() - start_time\n",
    "print(f\"‚ö° Setup completed in {setup_time:.2f} seconds!\")\n",
    "\n",
    "# Test cases for different types of questions\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What are the main features and goals of this employee onboarding project?\",\n",
    "        \"expected_route\": \"PRD\",\n",
    "        \"description\": \"Product requirements question\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What database technology was chosen and why?\", \n",
    "        \"expected_route\": \"Technical\",\n",
    "        \"description\": \"Technical architecture question\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What tables are defined in the database schema?\",\n",
    "        \"expected_route\": \"Technical\",\n",
    "        \"description\": \"Database schema question\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\nüß™ Running {len(test_cases)} test cases...\")\n",
    "\n",
    "# Run tests with timing\n",
    "total_test_time = 0\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n{'='*15} Test {i}: {test_case['description']} {'='*15}\")\n",
    "    print(f\"Question: {test_case['question']}\")\n",
    "    print(f\"Expected Route: {test_case['expected_route']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_start = time.time()\n",
    "    \n",
    "    # Run the multi-agent system\n",
    "    result = research_team_graph.invoke({\n",
    "        \"question\": test_case[\"question\"],\n",
    "        \"route_decision\": \"\",\n",
    "        \"documents\": [],\n",
    "        \"research_type\": \"\",\n",
    "        \"answer\": \"\"\n",
    "    })\n",
    "    \n",
    "    test_time = time.time() - test_start\n",
    "    total_test_time += test_time\n",
    "    \n",
    "    print(f\"\\nüìã RESULTS:\")\n",
    "    print(f\"  Route Taken: {result['route_decision'].upper()}\")\n",
    "    print(f\"  Research Type: {result['research_type']}\")\n",
    "    print(f\"  Documents Found: {len(result['documents'])}\")\n",
    "    print(f\"  ‚è±Ô∏è Execution Time: {test_time:.2f} seconds\")\n",
    "    \n",
    "    # Show document sources\n",
    "    if result['documents']:\n",
    "        unique_sources = list(set([doc.metadata.get('source', 'Unknown') for doc in result['documents']]))\n",
    "        print(f\"  Sources: {', '.join(unique_sources)}\")\n",
    "    \n",
    "    print(f\"\\nüìù Answer Preview:\")\n",
    "    print(f\"  {result['answer'][:100]}{'...' if len(result['answer']) > 100 else ''}\")\n",
    "    \n",
    "    # Validation\n",
    "    expected_lower = test_case['expected_route'].lower()\n",
    "    actual_lower = result['route_decision'].lower()\n",
    "    status = \"‚úÖ PASS\" if expected_lower == actual_lower else \"‚ùå FAIL\"\n",
    "    print(f\"\\nüéØ Routing Validation: {status}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìã FAST MULTI-AGENT RESEARCH TEAM SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"‚úÖ Lightning-fast system successfully implemented with:\")\n",
    "print(\"  ‚Ä¢ Intelligent rule-based routing (Project Manager)\")\n",
    "print(\"  ‚Ä¢ Filtered specialized retrievers (PRD vs Technical)\")\n",
    "print(\"  ‚Ä¢ Expert researchers (PRD & Technical specialists)\")\n",
    "print(\"  ‚Ä¢ Fast template-based synthesis\")\n",
    "print(\"  ‚Ä¢ Reuses existing knowledge base (no new embeddings!)\")\n",
    "print(f\"\\n‚ö° PERFORMANCE METRICS:\")\n",
    "print(f\"  ‚Ä¢ Total execution time: {total_time:.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Average per test: {total_test_time/len(test_cases):.2f} seconds\")\n",
    "print(f\"  ‚Ä¢ Setup time: {setup_time:.2f} seconds\")\n",
    "print(f\"\\nüéØ This is {300/total_time:.0f}x FASTER than the original 5+ minute version!\")\n",
    "print(\"‚ú® Challenge 3 Complete - Production-ready enterprise RAG system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Challenge: PRD-Specific RAG Graph for Your Application\n",
    "\n",
    "**Task:** Build a specialized RAG graph tailored for your AI-Powered Requirement Analyzer that can process user input and generate structured PRD content.\n",
    "\n",
    "This RAG system will be designed to integrate with your React application and provide the AI processing functionality needed to update PRDs in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRD-Specific RAG Graph for AI-Powered Requirement Analyzer\n",
    "\n",
    "from typing import List, TypedDict, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"üöÄ Building PRD-Specific RAG Graph for Your Application\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Define specialized state for PRD generation\n",
    "class PRDAgentState(TypedDict):\n",
    "    user_input: str  # Raw user description of their product idea\n",
    "    conversation_history: List[Dict[str, str]]  # Previous messages\n",
    "    retrieved_context: List[Document]  # Relevant examples from knowledge base\n",
    "    analysis_result: Dict[str, Any]  # Structured analysis of user input\n",
    "    prd_content: Dict[str, Any]  # Generated PRD sections\n",
    "    clarifying_questions: List[str]  # Questions to ask user for clarification\n",
    "    processing_stage: str  # Track current processing stage\n",
    "    error_message: str  # Any error messages\n",
    "\n",
    "# Step 2: Create specialized agents for PRD generation\n",
    "\n",
    "def input_analyzer_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Analyzes user input to identify key components for PRD generation.\"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "    \n",
    "    print(f\"üîç Analyzing user input: '{user_input[:50]}...'\")\n",
    "    \n",
    "    # Use retriever to find relevant examples and best practices\n",
    "    context_query = f\"product requirements examples features user stories {user_input}\"\n",
    "    retrieved_docs = retriever.invoke(context_query)\n",
    "    state[\"retrieved_context\"] = retrieved_docs\n",
    "    \n",
    "    # Analyze the input using LLM\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze the following user input for a product idea and extract key information:\n",
    "    \n",
    "    User Input: \"{user_input}\"\n",
    "    \n",
    "    Please identify and extract:\n",
    "    1. Product type/category\n",
    "    2. Main purpose/goal\n",
    "    3. Target users/personas\n",
    "    4. Key features mentioned\n",
    "    5. Technical requirements (if any)\n",
    "    6. Business objectives (if any)\n",
    "    7. Areas that need clarification\n",
    "    \n",
    "    Respond in JSON format with these keys: product_type, purpose, target_users, features, technical_requirements, business_objectives, needs_clarification\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert product analyst. Analyze user input and extract structured information for PRD generation. Always respond in valid JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        analysis_result = json.loads(response.choices[0].message.content)\n",
    "        state[\"analysis_result\"] = analysis_result\n",
    "        state[\"processing_stage\"] = \"analyzed\"\n",
    "        print(f\"‚úÖ Analysis complete - identified {len(analysis_result.get('features', []))} features\")\n",
    "    except json.JSONDecodeError:\n",
    "        state[\"error_message\"] = \"Failed to parse analysis result\"\n",
    "        state[\"processing_stage\"] = \"error\"\n",
    "        print(\"‚ùå Analysis failed - JSON parsing error\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def prd_generator_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Generates structured PRD content based on analysis.\"\"\"\n",
    "    analysis = state[\"analysis_result\"]\n",
    "    context_docs = state[\"retrieved_context\"]\n",
    "    \n",
    "    print(f\"üìù Generating PRD content...\")\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context_text = \"\\n\\n\".join([doc.page_content[:500] for doc in context_docs[:3]])\n",
    "    \n",
    "    # Generate PRD sections\n",
    "    prd_prompt = f\"\"\"\n",
    "    Based on the analysis and examples, generate structured PRD content:\n",
    "    \n",
    "    Analysis: {json.dumps(analysis, indent=2)}\n",
    "    \n",
    "    Reference Examples:\n",
    "    {context_text}\n",
    "    \n",
    "    Generate a comprehensive PRD with these sections:\n",
    "    1. title: A clear product title\n",
    "    2. overview: 2-3 sentence product summary\n",
    "    3. objectives: 3-5 specific business objectives\n",
    "    4. features: 5-8 key features with descriptions\n",
    "    5. requirements: Technical and non-functional requirements\n",
    "    6. user_stories: 3-5 user stories in \"As a [user], I want [feature], so that [benefit]\" format\n",
    "    7. success_metrics: 3-4 measurable KPIs\n",
    "    8. timeline: Suggested development phases\n",
    "    \n",
    "    Respond in JSON format with these exact keys. Make content specific to the analyzed product idea.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert product manager. Generate comprehensive PRD content based on analysis and examples. Always respond in valid JSON format with detailed, actionable content.\"},\n",
    "            {\"role\": \"user\", \"content\": prd_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        prd_content = json.loads(response.choices[0].message.content)\n",
    "        state[\"prd_content\"] = prd_content\n",
    "        state[\"processing_stage\"] = \"generated\"\n",
    "        print(f\"‚úÖ PRD generated with {len(prd_content)} sections\")\n",
    "    except json.JSONDecodeError:\n",
    "        state[\"error_message\"] = \"Failed to generate PRD content\"\n",
    "        state[\"processing_stage\"] = \"error\"\n",
    "        print(\"‚ùå PRD generation failed - JSON parsing error\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def clarification_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Generates clarifying questions to improve PRD quality.\"\"\"\n",
    "    analysis = state[\"analysis_result\"]\n",
    "    user_input = state[\"user_input\"]\n",
    "    \n",
    "    print(f\"‚ùì Generating clarifying questions...\")\n",
    "    \n",
    "    questions_prompt = f\"\"\"\n",
    "    Based on the user input and analysis, generate 2-4 clarifying questions to improve the PRD:\n",
    "    \n",
    "    User Input: \"{user_input}\"\n",
    "    Analysis: {json.dumps(analysis, indent=2)}\n",
    "    \n",
    "    Focus on areas that are:\n",
    "    1. Vague or underspecified\n",
    "    2. Missing important details\n",
    "    3. Could benefit from more specific requirements\n",
    "    4. Need user persona clarification\n",
    "    \n",
    "    Generate specific, actionable questions that will help create a better PRD.\n",
    "    Respond as a JSON array of question strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a product manager expert at asking clarifying questions. Generate specific questions that will improve PRD quality. Respond as a JSON array.\"},\n",
    "            {\"role\": \"user\", \"content\": questions_prompt}\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        questions = json.loads(response.choices[0].message.content)\n",
    "        state[\"clarifying_questions\"] = questions\n",
    "        print(f\"‚úÖ Generated {len(questions)} clarifying questions\")\n",
    "    except json.JSONDecodeError:\n",
    "        state[\"clarifying_questions\"] = [\"Could you provide more details about your target users?\"]\n",
    "        print(\"‚ö†Ô∏è Using default clarifying question\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "def quality_validator_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Validates and enhances the generated PRD quality.\"\"\"\n",
    "    prd_content = state.get(\"prd_content\", {})\n",
    "    \n",
    "    print(f\"üîç Validating PRD quality...\")\n",
    "    \n",
    "    if not prd_content:\n",
    "        state[\"processing_stage\"] = \"validation_failed\"\n",
    "        state[\"error_message\"] = \"No PRD content to validate\"\n",
    "        return state\n",
    "    \n",
    "    # Basic quality checks\n",
    "    required_sections = [\"title\", \"overview\", \"objectives\", \"features\", \"user_stories\"]\n",
    "    missing_sections = [section for section in required_sections if not prd_content.get(section)]\n",
    "    \n",
    "    if missing_sections:\n",
    "        print(f\"‚ö†Ô∏è Missing sections: {missing_sections}\")\n",
    "        # Add placeholder content for missing sections\n",
    "        for section in missing_sections:\n",
    "            if section == \"title\":\n",
    "                prd_content[section] = \"Product Requirements Document\"\n",
    "            elif section == \"overview\":\n",
    "                prd_content[section] = \"This product aims to solve key user problems through innovative features.\"\n",
    "            elif section == \"objectives\":\n",
    "                prd_content[section] = [\"Define clear business objectives\", \"Identify target market\", \"Establish success metrics\"]\n",
    "            elif section == \"features\":\n",
    "                prd_content[section] = [\"Core functionality\", \"User interface\", \"Basic integrations\"]\n",
    "            elif section == \"user_stories\":\n",
    "                prd_content[section] = [\"As a user, I want basic functionality, so that I can achieve my goals\"]\n",
    "    \n",
    "    # Ensure features and user_stories are lists\n",
    "    if isinstance(prd_content.get(\"features\"), str):\n",
    "        prd_content[\"features\"] = [prd_content[\"features\"]]\n",
    "    if isinstance(prd_content.get(\"user_stories\"), str):\n",
    "        prd_content[\"user_stories\"] = [prd_content[\"user_stories\"]]\n",
    "    \n",
    "    state[\"prd_content\"] = prd_content\n",
    "    state[\"processing_stage\"] = \"validated\"\n",
    "    print(f\"‚úÖ PRD validation complete\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Step 3: Create routing logic\n",
    "def determine_next_step(state: PRDAgentState) -> str:\n",
    "    \"\"\"Determines the next processing step based on current stage.\"\"\"\n",
    "    stage = state.get(\"processing_stage\", \"\")\n",
    "    \n",
    "    if stage == \"analyzed\":\n",
    "        return \"generate_prd\"\n",
    "    elif stage == \"generated\":\n",
    "        return \"validate_quality\"\n",
    "    elif stage == \"validated\":\n",
    "        return \"generate_questions\"\n",
    "    elif stage == \"error\":\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Step 4: Build the PRD-specific RAG graph\n",
    "def create_prd_rag_graph():\n",
    "    \"\"\"Creates the specialized PRD generation RAG graph.\"\"\"\n",
    "    \n",
    "    workflow = StateGraph(PRDAgentState)\n",
    "    \n",
    "    # Add agent nodes\n",
    "    workflow.add_node(\"analyze_input\", input_analyzer_agent)\n",
    "    workflow.add_node(\"generate_prd\", prd_generator_agent)\n",
    "    workflow.add_node(\"validate_quality\", quality_validator_agent)\n",
    "    workflow.add_node(\"generate_questions\", clarification_agent)\n",
    "    \n",
    "    # Define workflow\n",
    "    workflow.set_entry_point(\"analyze_input\")\n",
    "    \n",
    "    # Add conditional edges based on processing stage\n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_input\",\n",
    "        determine_next_step,\n",
    "        {\n",
    "            \"generate_prd\": \"generate_prd\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_prd\",\n",
    "        determine_next_step,\n",
    "        {\n",
    "            \"validate_quality\": \"validate_quality\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"validate_quality\",\n",
    "        determine_next_step,\n",
    "        {\n",
    "            \"generate_questions\": \"generate_questions\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"generate_questions\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Step 5: Create utility function for App.js integration\n",
    "def process_user_input_for_prd(user_input: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to process user input and generate PRD content.\n",
    "    This function can be called from your FastAPI backend to integrate with App.js.\n",
    "    \n",
    "    Args:\n",
    "        user_input: User's description of their product idea\n",
    "        conversation_history: Previous conversation messages (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing PRD content, questions, and processing status\n",
    "    \"\"\"\n",
    "    \n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"user_input\": user_input,\n",
    "        \"conversation_history\": conversation_history,\n",
    "        \"retrieved_context\": [],\n",
    "        \"analysis_result\": {},\n",
    "        \"prd_content\": {},\n",
    "        \"clarifying_questions\": [],\n",
    "        \"processing_stage\": \"starting\",\n",
    "        \"error_message\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the PRD RAG graph\n",
    "    result = prd_rag_graph.invoke(initial_state)\n",
    "    \n",
    "    # Format response for frontend\n",
    "    response = {\n",
    "        \"success\": result[\"processing_stage\"] != \"error\",\n",
    "        \"prd_content\": result.get(\"prd_content\", {}),\n",
    "        \"clarifying_questions\": result.get(\"clarifying_questions\", []),\n",
    "        \"analysis\": result.get(\"analysis_result\", {}),\n",
    "        \"error_message\": result.get(\"error_message\", \"\"),\n",
    "        \"processing_stage\": result.get(\"processing_stage\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Step 6: Initialize and test the PRD RAG system\n",
    "print(\"üîß Initializing PRD-specific RAG graph...\")\n",
    "prd_rag_graph = create_prd_rag_graph()\n",
    "print(\"‚úÖ PRD RAG graph ready!\")\n",
    "\n",
    "# Test with sample inputs similar to what users might provide\n",
    "test_inputs = [\n",
    "    \"I want to build a mobile app for tracking daily habits and goals\",\n",
    "    \"Create a web platform for small businesses to manage their inventory and sales\",\n",
    "    \"Build an AI-powered customer service chatbot for e-commerce websites\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüß™ Testing PRD RAG system with {len(test_inputs)} sample inputs...\")\n",
    "\n",
    "for i, test_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n{'='*15} Test {i}: PRD Generation {'='*15}\")\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Process the input\n",
    "    result = process_user_input_for_prd(test_input)\n",
    "    \n",
    "    print(f\"üìä Processing Result:\")\n",
    "    print(f\"  Success: {result['success']}\")\n",
    "    print(f\"  Stage: {result['processing_stage']}\")\n",
    "    \n",
    "    if result['success']:\n",
    "        prd = result['prd_content']\n",
    "        print(f\"  Generated PRD Sections: {list(prd.keys())}\")\n",
    "        print(f\"  Title: {prd.get('title', 'N/A')}\")\n",
    "        print(f\"  Features Count: {len(prd.get('features', []))}\")\n",
    "        print(f\"  User Stories Count: {len(prd.get('user_stories', []))}\")\n",
    "        print(f\"  Clarifying Questions: {len(result['clarifying_questions'])}\")\n",
    "        \n",
    "        # Show first feature and user story as examples\n",
    "        if prd.get('features'):\n",
    "            print(f\"  Sample Feature: {prd['features'][0]}\")\n",
    "        if prd.get('user_stories'):\n",
    "            print(f\"  Sample User Story: {prd['user_stories'][0]}\")\n",
    "    else:\n",
    "        print(f\"  Error: {result['error_message']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ PRD-Specific RAG System Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"‚úÖ Custom RAG system built for your application with:\")\n",
    "print(\"  ‚Ä¢ Input analysis and feature extraction\")\n",
    "print(\"  ‚Ä¢ Structured PRD content generation\")\n",
    "print(\"  ‚Ä¢ Quality validation and enhancement\")\n",
    "print(\"  ‚Ä¢ Clarifying question generation\")\n",
    "print(\"  ‚Ä¢ Integration-ready API function\")\n",
    "print(\"  ‚Ä¢ Error handling and validation\")\n",
    "print(f\"\\nüì± Integration with App.js:\")\n",
    "print(\"  ‚Ä¢ Use process_user_input_for_prd() in your FastAPI backend\")\n",
    "print(\"  ‚Ä¢ Returns structured JSON for frontend consumption\")\n",
    "print(\"  ‚Ä¢ Handles conversation history for iterative improvement\")\n",
    "print(\"  ‚Ä¢ Provides clarifying questions for better user experience\")\n",
    "print(f\"\\nüöÄ Ready for production integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Guide: Connecting RAG to Your Application\n",
    "\n",
    "Now that you have a specialized PRD RAG system, here's how to integrate it with your existing FastAPI backend and React frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Code Examples for Your Application\n",
    "\n",
    "print(\"üîß FastAPI Backend Integration Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: FastAPI endpoint for PRD processing\n",
    "fastapi_endpoint_example = '''\n",
    "# Add this to your main.py FastAPI application\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Request/Response models for PRD processing\n",
    "class PRDRequest(BaseModel):\n",
    "    user_input: str\n",
    "    conversation_history: Optional[List[Dict[str, str]]] = []\n",
    "\n",
    "class PRDResponse(BaseModel):\n",
    "    success: bool\n",
    "    prd_content: Dict[str, Any]\n",
    "    clarifying_questions: List[str]\n",
    "    analysis: Dict[str, Any]\n",
    "    error_message: str = \"\"\n",
    "    processing_stage: str\n",
    "\n",
    "@app.post(\"/api/process-prd\", response_model=PRDResponse)\n",
    "async def process_prd_input(request: PRDRequest):\n",
    "    \"\"\"\n",
    "    Process user input and generate PRD content using RAG system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import your RAG function (from this notebook or separate module)\n",
    "        # from rag_system import process_user_input_for_prd\n",
    "        \n",
    "        result = process_user_input_for_prd(\n",
    "            user_input=request.user_input,\n",
    "            conversation_history=request.conversation_history\n",
    "        )\n",
    "        \n",
    "        return PRDResponse(**result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return PRDResponse(\n",
    "            success=False,\n",
    "            prd_content={},\n",
    "            clarifying_questions=[],\n",
    "            analysis={},\n",
    "            error_message=str(e),\n",
    "            processing_stage=\"error\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/api/refine-prd\")\n",
    "async def refine_prd(request: PRDRequest):\n",
    "    \"\"\"\n",
    "    Refine existing PRD based on additional user input.\n",
    "    \"\"\"\n",
    "    # Similar implementation but with refinement logic\n",
    "    pass\n",
    "'''\n",
    "\n",
    "print(\"üìù FastAPI Endpoint Code:\")\n",
    "print(fastapi_endpoint_example)\n",
    "\n",
    "# Example 2: Enhanced React App.js integration\n",
    "react_integration_example = '''\n",
    "// Enhanced handleSendMessage function for App.js\n",
    "\n",
    "const handleSendMessage = async () => {\n",
    "  if (!input.trim()) return;\n",
    "\n",
    "  const userMessage = {\n",
    "    id: messages.length + 1,\n",
    "    type: 'user',\n",
    "    content: input\n",
    "  };\n",
    "\n",
    "  setMessages(prev => [...prev, userMessage]);\n",
    "  const currentInput = input;\n",
    "  setInput('');\n",
    "  setIsGenerating(true);\n",
    "\n",
    "  try {\n",
    "    // Call your FastAPI backend\n",
    "    const response = await fetch('/api/process-prd', {\n",
    "      method: 'POST',\n",
    "      headers: {\n",
    "        'Content-Type': 'application/json',\n",
    "      },\n",
    "      body: JSON.stringify({\n",
    "        user_input: currentInput,\n",
    "        conversation_history: messages.map(msg => ({\n",
    "          role: msg.type === 'user' ? 'user' : 'assistant',\n",
    "          content: msg.content\n",
    "        }))\n",
    "      })\n",
    "    });\n",
    "\n",
    "    const result = await response.json();\n",
    "\n",
    "    if (result.success) {\n",
    "      // Update PRD content with AI-generated sections\n",
    "      setPrdContent(prev => ({\n",
    "        ...prev,\n",
    "        title: result.prd_content.title || prev.title,\n",
    "        overview: result.prd_content.overview || prev.overview,\n",
    "        objectives: result.prd_content.objectives || prev.objectives,\n",
    "        features: result.prd_content.features || prev.features,\n",
    "        requirements: result.prd_content.requirements || prev.requirements,\n",
    "        userStories: result.prd_content.user_stories || prev.userStories\n",
    "      }));\n",
    "\n",
    "      // Add AI response with clarifying questions\n",
    "      let aiResponse = \"I've analyzed your input and updated the PRD. \";\n",
    "      if (result.clarifying_questions.length > 0) {\n",
    "        aiResponse += \"Here are some questions to help me improve it further:\\\\n\\\\n\";\n",
    "        result.clarifying_questions.forEach((q, i) => {\n",
    "          aiResponse += `${i + 1}. ${q}\\\\n`;\n",
    "        });\n",
    "      }\n",
    "\n",
    "      const assistantMessage = {\n",
    "        id: messages.length + 2,\n",
    "        type: 'assistant',\n",
    "        content: aiResponse\n",
    "      };\n",
    "\n",
    "      setMessages(prev => [...prev, assistantMessage]);\n",
    "    } else {\n",
    "      // Handle error\n",
    "      const errorMessage = {\n",
    "        id: messages.length + 2,\n",
    "        type: 'assistant',\n",
    "        content: `Sorry, I encountered an error: ${result.error_message}`\n",
    "      };\n",
    "      setMessages(prev => [...prev, errorMessage]);\n",
    "    }\n",
    "\n",
    "  } catch (error) {\n",
    "    console.error('Error calling PRD API:', error);\n",
    "    const errorMessage = {\n",
    "      id: messages.length + 2,\n",
    "      type: 'assistant',\n",
    "      content: 'Sorry, I encountered a technical error. Please try again.'\n",
    "    };\n",
    "    setMessages(prev => [...prev, errorMessage]);\n",
    "  } finally {\n",
    "    setIsGenerating(false);\n",
    "  }\n",
    "};\n",
    "'''\n",
    "\n",
    "print(\"\\nüé® React Integration Code:\")\n",
    "print(react_integration_example)\n",
    "\n",
    "# Example 3: Deployment considerations\n",
    "deployment_notes = '''\n",
    "# Deployment and Production Considerations\n",
    "\n",
    "## 1. Environment Setup\n",
    "- Install required packages: langgraph, langchain, faiss-cpu, openai\n",
    "- Set up environment variables for API keys\n",
    "- Configure vector store persistence\n",
    "\n",
    "## 2. Performance Optimization\n",
    "- Cache embeddings and vector stores\n",
    "- Implement request rate limiting\n",
    "- Use async processing for long operations\n",
    "- Consider GPU acceleration for large deployments\n",
    "\n",
    "## 3. Error Handling\n",
    "- Implement comprehensive error logging\n",
    "- Add retry mechanisms for API calls\n",
    "- Validate user input before processing\n",
    "- Handle API rate limits gracefully\n",
    "\n",
    "## 4. Security\n",
    "- Validate and sanitize all user inputs\n",
    "- Implement proper authentication\n",
    "- Use HTTPS for all API communications\n",
    "- Store API keys securely\n",
    "\n",
    "## 5. Monitoring\n",
    "- Track API response times\n",
    "- Monitor LLM token usage\n",
    "- Log user interactions for improvement\n",
    "- Set up health checks for all services\n",
    "'''\n",
    "\n",
    "print(\"\\nüöÄ Deployment Notes:\")\n",
    "print(deployment_notes)\n",
    "\n",
    "# Example 4: Testing the integration\n",
    "print(\"\\nüß™ Testing Your Integration:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Simulate API call to test the integration\n",
    "test_api_request = {\n",
    "    \"user_input\": \"I want to build a task management app for remote teams\",\n",
    "    \"conversation_history\": []\n",
    "}\n",
    "\n",
    "print(\"Sample API Request:\")\n",
    "print(json.dumps(test_api_request, indent=2))\n",
    "\n",
    "# Process with our RAG system\n",
    "test_result = process_user_input_for_prd(\n",
    "    test_api_request[\"user_input\"], \n",
    "    test_api_request[\"conversation_history\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSample API Response:\")\n",
    "print(json.dumps({\n",
    "    \"success\": test_result[\"success\"],\n",
    "    \"prd_content\": {\n",
    "        \"title\": test_result[\"prd_content\"].get(\"title\", \"\"),\n",
    "        \"features_count\": len(test_result[\"prd_content\"].get(\"features\", [])),\n",
    "        \"user_stories_count\": len(test_result[\"prd_content\"].get(\"user_stories\", []))\n",
    "    },\n",
    "    \"clarifying_questions_count\": len(test_result[\"clarifying_questions\"]),\n",
    "    \"processing_stage\": test_result[\"processing_stage\"]\n",
    "}, indent=2))\n",
    "\n",
    "print(f\"\\n‚úÖ Integration Examples Complete!\")\n",
    "print(\"üìã Next Steps:\")\n",
    "print(\"  1. Copy the FastAPI endpoint code to your main.py\")\n",
    "print(\"  2. Update your React App.js with the enhanced handleSendMessage\")\n",
    "print(\"  3. Test the integration with sample inputs\")\n",
    "print(\"  4. Deploy and monitor the system\")\n",
    "print(\"  5. Iterate based on user feedback\")\n",
    "\n",
    "# Save the RAG function to a separate file for easy import\n",
    "rag_module_code = '''\n",
    "# Save this as rag_system.py in your project root\n",
    "\n",
    "# [Include all the RAG system code from the previous cell]\n",
    "# This allows you to import: from rag_system import process_user_input_for_prd\n",
    "'''\n",
    "\n",
    "print(f\"\\nüí° Pro Tip: Save the RAG system code as a separate Python module\")\n",
    "print(\"   for easier import and maintenance in your FastAPI application.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Integration for Your App.js\n",
    "\n",
    "Here's the complete integration process to connect your RAG system with your existing React application structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Create a RAG module for your FastAPI backend\n",
    "\n",
    "print(\"üöÄ Creating RAG System Integration for Your Application\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's create the RAG system code that you'll save as a separate file\n",
    "rag_system_code = '''\n",
    "# Save this as: rag_system.py in your project root directory\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from typing import List, TypedDict, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "\n",
    "# Import your utils\n",
    "from utils import setup_llm_client\n",
    "\n",
    "# Initialize the LLM client\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")\n",
    "\n",
    "# Create knowledge base (you can enhance this with more documents)\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Creates the knowledge base for PRD generation.\"\"\"\n",
    "    artifact_paths = [\"artifacts/prd_gen.md\", \"artifacts/schema.sql\", \"artifacts/adr.md\"]\n",
    "    all_docs = []\n",
    "    \n",
    "    for path in artifact_paths:\n",
    "        if os.path.exists(path):\n",
    "            loader = TextLoader(path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata = {\"source\": path}\n",
    "            all_docs.extend(docs)\n",
    "    \n",
    "    if not all_docs:\n",
    "        # Create a minimal knowledge base with PRD examples\n",
    "        example_doc = Document(\n",
    "            page_content=\"\"\"\n",
    "            Product Requirements Document Template:\n",
    "            1. Executive Summary: Brief overview of the product\n",
    "            2. Objectives: Clear business goals and success metrics\n",
    "            3. Features: Key functionalities and capabilities\n",
    "            4. User Stories: As a [user], I want [feature], so that [benefit]\n",
    "            5. Technical Requirements: Technology stack and infrastructure needs\n",
    "            \"\"\",\n",
    "            metadata={\"source\": \"template\"}\n",
    "        )\n",
    "        all_docs = [example_doc]\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = create_knowledge_base()\n",
    "\n",
    "# PRD Agent State\n",
    "class PRDAgentState(TypedDict):\n",
    "    user_input: str\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    retrieved_context: List[Document]\n",
    "    analysis_result: Dict[str, Any]\n",
    "    prd_content: Dict[str, Any]\n",
    "    clarifying_questions: List[str]\n",
    "    processing_stage: str\n",
    "    error_message: str\n",
    "\n",
    "# Agent functions\n",
    "def input_analyzer_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Analyzes user input for PRD generation.\"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "    \n",
    "    # Retrieve relevant context\n",
    "    context_query = f\"product requirements examples features user stories {user_input}\"\n",
    "    retrieved_docs = retriever.invoke(context_query)\n",
    "    state[\"retrieved_context\"] = retrieved_docs\n",
    "    \n",
    "    # Analyze input\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze this product idea: \"{user_input}\"\n",
    "    \n",
    "    Extract:\n",
    "    1. Product type/category\n",
    "    2. Main purpose/goal  \n",
    "    3. Target users\n",
    "    4. Key features mentioned\n",
    "    5. Technical requirements\n",
    "    6. Business objectives\n",
    "    \n",
    "    Respond in JSON: {{\"product_type\": \"\", \"purpose\": \"\", \"target_users\": [], \"features\": [], \"technical_requirements\": [], \"business_objectives\": []}}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Extract structured information from product ideas. Respond only with valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        analysis_result = json.loads(response.choices[0].message.content)\n",
    "        state[\"analysis_result\"] = analysis_result\n",
    "        state[\"processing_stage\"] = \"analyzed\"\n",
    "    except:\n",
    "        state[\"error_message\"] = \"Analysis failed\"\n",
    "        state[\"processing_stage\"] = \"error\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def prd_generator_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Generates PRD content matching your App.js structure.\"\"\"\n",
    "    analysis = state[\"analysis_result\"]\n",
    "    \n",
    "    # Create PRD content that matches your React component structure exactly\n",
    "    prd_prompt = f\"\"\"\n",
    "    Based on this analysis: {json.dumps(analysis, indent=2)}\n",
    "    \n",
    "    Generate a PRD with these EXACT fields to match the React app structure:\n",
    "    \n",
    "    {{\n",
    "        \"title\": \"A clear, compelling product title\",\n",
    "        \"overview\": \"2-3 sentences describing the product and its value proposition\",\n",
    "        \"objectives\": [\"Business objective 1\", \"Business objective 2\", \"Business objective 3\"],\n",
    "        \"features\": [\"Feature 1: Description\", \"Feature 2: Description\", \"Feature 3: Description\"],\n",
    "        \"requirements\": [\"Technical requirement 1\", \"Technical requirement 2\", \"Technical requirement 3\"],\n",
    "        \"userStories\": [\"As a user, I want X, so that Y\", \"As a user, I want A, so that B\"]\n",
    "    }}\n",
    "    \n",
    "    Make it specific to the analyzed product idea. Respond with valid JSON only.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate PRD content as valid JSON matching the exact field structure provided.\"},\n",
    "                {\"role\": \"user\", \"content\": prd_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        prd_content = json.loads(response.choices[0].message.content)\n",
    "        \n",
    "        # Ensure all fields exist and are correct types\n",
    "        prd_content.setdefault(\"title\", \"Product Requirements Document\")\n",
    "        prd_content.setdefault(\"overview\", \"Product overview will be generated from your input.\")\n",
    "        prd_content.setdefault(\"objectives\", [])\n",
    "        prd_content.setdefault(\"features\", [])\n",
    "        prd_content.setdefault(\"requirements\", [])\n",
    "        prd_content.setdefault(\"userStories\", [])\n",
    "        \n",
    "        # Ensure arrays are actually arrays\n",
    "        for field in [\"objectives\", \"features\", \"requirements\", \"userStories\"]:\n",
    "            if not isinstance(prd_content[field], list):\n",
    "                prd_content[field] = [str(prd_content[field])] if prd_content[field] else []\n",
    "        \n",
    "        state[\"prd_content\"] = prd_content\n",
    "        state[\"processing_stage\"] = \"generated\"\n",
    "    except Exception as e:\n",
    "        state[\"error_message\"] = f\"PRD generation failed: {str(e)}\"\n",
    "        state[\"processing_stage\"] = \"error\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def clarification_agent(state: PRDAgentState) -> PRDAgentState:\n",
    "    \"\"\"Generates clarifying questions.\"\"\"\n",
    "    user_input = state[\"user_input\"]\n",
    "    \n",
    "    questions_prompt = f\"\"\"\n",
    "    For this product idea: \"{user_input}\"\n",
    "    \n",
    "    Generate 2-3 specific clarifying questions to improve the PRD.\n",
    "    Focus on missing details about users, features, or requirements.\n",
    "    \n",
    "    Respond as JSON array: [\"Question 1?\", \"Question 2?\", \"Question 3?\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Generate clarifying questions as a JSON array.\"},\n",
    "                {\"role\": \"user\", \"content\": questions_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        questions = json.loads(response.choices[0].message.content)\n",
    "        state[\"clarifying_questions\"] = questions\n",
    "    except:\n",
    "        state[\"clarifying_questions\"] = [\"Could you provide more details about your target users?\"]\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Routing logic\n",
    "def determine_next_step(state: PRDAgentState) -> str:\n",
    "    stage = state.get(\"processing_stage\", \"\")\n",
    "    if stage == \"analyzed\":\n",
    "        return \"generate_prd\"\n",
    "    elif stage == \"generated\":\n",
    "        return \"generate_questions\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "# Create the graph\n",
    "def create_prd_rag_graph():\n",
    "    workflow = StateGraph(PRDAgentState)\n",
    "    \n",
    "    workflow.add_node(\"analyze_input\", input_analyzer_agent)\n",
    "    workflow.add_node(\"generate_prd\", prd_generator_agent)\n",
    "    workflow.add_node(\"generate_questions\", clarification_agent)\n",
    "    \n",
    "    workflow.set_entry_point(\"analyze_input\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"analyze_input\",\n",
    "        determine_next_step,\n",
    "        {\"generate_prd\": \"generate_prd\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"generate_prd\", \n",
    "        determine_next_step,\n",
    "        {\"generate_questions\": \"generate_questions\", \"end\": END}\n",
    "    )\n",
    "    \n",
    "    workflow.add_edge(\"generate_questions\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Initialize the graph\n",
    "prd_rag_graph = create_prd_rag_graph()\n",
    "\n",
    "# Main function for API integration\n",
    "def process_user_input_for_prd(user_input: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to process user input and generate PRD content.\n",
    "    Returns data structure that matches your React app exactly.\n",
    "    \"\"\"\n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_input\": user_input,\n",
    "        \"conversation_history\": conversation_history,\n",
    "        \"retrieved_context\": [],\n",
    "        \"analysis_result\": {},\n",
    "        \"prd_content\": {},\n",
    "        \"clarifying_questions\": [],\n",
    "        \"processing_stage\": \"starting\",\n",
    "        \"error_message\": \"\"\n",
    "    }\n",
    "    \n",
    "    result = prd_rag_graph.invoke(initial_state)\n",
    "    \n",
    "    return {\n",
    "        \"success\": result[\"processing_stage\"] != \"error\",\n",
    "        \"prd_content\": result.get(\"prd_content\", {}),\n",
    "        \"clarifying_questions\": result.get(\"clarifying_questions\", []),\n",
    "        \"analysis\": result.get(\"analysis_result\", {}),\n",
    "        \"error_message\": result.get(\"error_message\", \"\"),\n",
    "        \"processing_stage\": result.get(\"processing_stage\", \"unknown\")\n",
    "    }\n",
    "'''\n",
    "\n",
    "# Save the instruction for creating the file\n",
    "print(\"üìÅ STEP 1: Save RAG System Module\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Save the code above as 'rag_system.py' in your project root directory.\")\n",
    "print(\"This module contains all the RAG logic optimized for your App.js structure.\")\n",
    "print()\n",
    "\n",
    "# STEP 2: FastAPI Integration\n",
    "fastapi_integration_code = '''\n",
    "# STEP 2: Add this to your main.py FastAPI application\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel\n",
    "from rag_system import process_user_input_for_prd\n",
    "\n",
    "# Add these models to your existing main.py\n",
    "class PRDRequest(BaseModel):\n",
    "    user_input: str\n",
    "    conversation_history: Optional[List[Dict[str, str]]] = []\n",
    "\n",
    "class PRDResponse(BaseModel):\n",
    "    success: bool\n",
    "    prd_content: Dict[str, Any]\n",
    "    clarifying_questions: List[str]\n",
    "    analysis: Dict[str, Any]\n",
    "    error_message: str = \"\"\n",
    "    processing_stage: str\n",
    "\n",
    "# Add this endpoint to your existing FastAPI app\n",
    "@app.post(\"/api/process-prd\", response_model=PRDResponse)\n",
    "async def process_prd_input(request: PRDRequest):\n",
    "    \"\"\"\n",
    "    Process user input and generate PRD content using RAG system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = process_user_input_for_prd(\n",
    "            user_input=request.user_input,\n",
    "            conversation_history=request.conversation_history\n",
    "        )\n",
    "        \n",
    "        return PRDResponse(**result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return PRDResponse(\n",
    "            success=False,\n",
    "            prd_content={},\n",
    "            clarifying_questions=[],\n",
    "            analysis={},\n",
    "            error_message=str(e),\n",
    "            processing_stage=\"error\"\n",
    "        )\n",
    "'''\n",
    "\n",
    "print(\"üì° STEP 2: FastAPI Backend Integration\")\n",
    "print(\"-\" * 40)\n",
    "print(fastapi_integration_code)\n",
    "print()\n",
    "\n",
    "# STEP 3: React App.js Integration\n",
    "react_integration_code = '''\n",
    "// STEP 3: Update your App.js handleSendMessage function\n",
    "\n",
    "const handleSendMessage = async () => {\n",
    "  if (!input.trim()) return;\n",
    "\n",
    "  const userMessage = {\n",
    "    id: messages.length + 1,\n",
    "    type: 'user',\n",
    "    content: input\n",
    "  };\n",
    "\n",
    "  setMessages(prev => [...prev, userMessage]);\n",
    "  const currentInput = input;\n",
    "  setInput('');\n",
    "  setIsGenerating(true);\n",
    "\n",
    "  try {\n",
    "    // Call your FastAPI backend\n",
    "    const response = await fetch('/api/process-prd', {\n",
    "      method: 'POST',\n",
    "      headers: {\n",
    "        'Content-Type': 'application/json',\n",
    "      },\n",
    "      body: JSON.stringify({\n",
    "        user_input: currentInput,\n",
    "        conversation_history: messages.map(msg => ({\n",
    "          role: msg.type === 'user' ? 'user' : 'assistant',\n",
    "          content: msg.content\n",
    "        }))\n",
    "      })\n",
    "    });\n",
    "\n",
    "    const result = await response.json();\n",
    "\n",
    "    if (result.success) {\n",
    "      // Update PRD content with AI-generated sections\n",
    "      // This directly populates your existing prdContent state structure\n",
    "      setPrdContent(prev => ({\n",
    "        ...prev,\n",
    "        title: result.prd_content.title || prev.title,\n",
    "        overview: result.prd_content.overview || prev.overview,\n",
    "        objectives: result.prd_content.objectives || prev.objectives,\n",
    "        features: result.prd_content.features || prev.features,\n",
    "        requirements: result.prd_content.requirements || prev.requirements,\n",
    "        userStories: result.prd_content.userStories || prev.userStories\n",
    "      }));\n",
    "\n",
    "      // Create AI response with clarifying questions\n",
    "      let aiResponse = \"I've analyzed your product idea and updated the PRD! \";\n",
    "      \n",
    "      if (result.clarifying_questions.length > 0) {\n",
    "        aiResponse += \"\\\\n\\\\nTo make it even better, could you help me with these questions:\\\\n\";\n",
    "        result.clarifying_questions.forEach((q, i) => {\n",
    "          aiResponse += `\\\\n${i + 1}. ${q}`;\n",
    "        });\n",
    "      } else {\n",
    "        aiResponse += \"The PRD looks complete based on your input.\";\n",
    "      }\n",
    "\n",
    "      const assistantMessage = {\n",
    "        id: messages.length + 2,\n",
    "        type: 'assistant',\n",
    "        content: aiResponse\n",
    "      };\n",
    "\n",
    "      setMessages(prev => [...prev, assistantMessage]);\n",
    "    } else {\n",
    "      // Handle error\n",
    "      const errorMessage = {\n",
    "        id: messages.length + 2,\n",
    "        type: 'assistant',\n",
    "        content: `Sorry, I encountered an error: ${result.error_message}`\n",
    "      };\n",
    "      setMessages(prev => [...prev, errorMessage]);\n",
    "    }\n",
    "\n",
    "  } catch (error) {\n",
    "    console.error('Error calling PRD API:', error);\n",
    "    const errorMessage = {\n",
    "      id: messages.length + 2,\n",
    "      type: 'assistant',\n",
    "      content: 'Sorry, I encountered a technical error. Please try again.'\n",
    "    };\n",
    "    setMessages(prev => [...prev, errorMessage]);\n",
    "  } finally {\n",
    "    setIsGenerating(false);\n",
    "  }\n",
    "};\n",
    "'''\n",
    "\n",
    "print(\"‚öõÔ∏è STEP 3: React App.js Integration\")\n",
    "print(\"-\" * 40)\n",
    "print(react_integration_code)\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ INTEGRATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ Your RAG system is now perfectly aligned with your App.js structure!\")\n",
    "print()\n",
    "print(\"üìã What happens when a user types an idea:\")\n",
    "print(\"  1. User types: 'I want to build a habit tracking app'\")\n",
    "print(\"  2. RAG system analyzes the input and generates structured PRD content\")\n",
    "print(\"  3. Your React app receives the exact data structure it expects:\")\n",
    "print(\"     ‚Ä¢ title: 'Habit Tracking Mobile Application'\") \n",
    "print(\"     ‚Ä¢ overview: 'A mobile app that helps users...'\")\n",
    "print(\"     ‚Ä¢ objectives: ['Increase user engagement', 'Track daily habits']\")\n",
    "print(\"     ‚Ä¢ features: ['Daily habit logging', 'Progress visualization']\")\n",
    "print(\"     ‚Ä¢ requirements: ['Mobile-responsive design', 'Data persistence']\")\n",
    "print(\"     ‚Ä¢ userStories: ['As a user, I want to log habits, so that I can track progress']\")\n",
    "print(\"  4. Your PRD sections automatically populate in real-time!\")\n",
    "print()\n",
    "print(\"üöÄ Next Steps:\")\n",
    "print(\"  1. Save the rag_system.py file in your project root\")\n",
    "print(\"  2. Add the FastAPI endpoint to your main.py\")\n",
    "print(\"  3. Update your App.js handleSendMessage function\")\n",
    "print(\"  4. Test with sample inputs!\")\n",
    "print()\n",
    "print(\"üí° The system is optimized to generate content that matches your exact\")\n",
    "print(\"   React component structure, so no additional mapping is needed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
