{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\labadmin\\Desktop\\220372-AG-AISOFTDEV-Team-3-PromptPioneers\\LABS\n",
      "Project root: c:\\Users\\labadmin\\Desktop\\220372-AG-AISOFTDEV-Team-3-PromptPioneers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:57:25,602 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4.1 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded main.py from c:\\Users\\labadmin\\Desktop\\220372-AG-AISOFTDEV-Team-3-PromptPioneers\\main.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "# The project root should be 220372-AG-AISOFTDEV-Team-3-PromptPioneers\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Also add current directory\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")\n",
    "\n",
    "# Load the application code from the root folder main.py to provide context for test generation\n",
    "main_py_path = os.path.join(project_root, \"main.py\")\n",
    "try:\n",
    "    with open(main_py_path, 'r', encoding='utf-8') as f:\n",
    "        app_code = f.read()\n",
    "    print(f\"Successfully loaded main.py from {main_py_path}\")\n",
    "except FileNotFoundError:\n",
    "    app_code = None\n",
    "    print(f\"Warning: Could not find main.py at {main_py_path}. Lab may not function correctly.\")\n",
    "except Exception as e:\n",
    "    app_code = None\n",
    "    print(f\"Warning: Error loading main.py: {e}. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "\"\"\"\n",
      "Pytest Test Suite for File Attachment CRUD Endpoints\n",
      "\n",
      "This suite contains happy path tests for the main CRUD operations\n",
      "of the AI-Powered Requirement Analyzer's file attachment API.\n",
      "\n",
      "Requirements:\n",
      "- Tests are independent and self-contained.\n",
      "- Each test uses realistic data and validates status codes & response content.\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from datetime import datetime\n",
      "\n",
      "from main import app\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Helper: Default payload for file attachments\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def get_file_attachment_payload(\n",
      "    conversation_id=1,\n",
      "    original_filename=\"requirements.docx\",\n",
      "    file_type=\"docx\",\n",
      "    file_size=12345,\n",
      "    storage_path=\"/storage/reqs/requirements.docx\",\n",
      "    extracted_text=\"Extracted requirement text.\",\n",
      "    status=\"uploaded\",\n",
      "    additional_metadata='{\"author\": \"Alice\"}'\n",
      "):\n",
      "    return {\n",
      "        \"conversation_id\": conversation_id,\n",
      "        \"original_filename\": original_filename,\n",
      "        \"file_type\": file_type,\n",
      "        \"file_size\": file_size,\n",
      "        \"storage_path\": storage_path,\n",
      "        \"extracted_text\": extracted_text,\n",
      "        \"status\": status,\n",
      "        \"additional_metadata\": additional_metadata,\n",
      "    }\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: POST /file-attachments/\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_create_file_attachment():\n",
      "    \"\"\"\n",
      "    Test successful creation of a file attachment.\n",
      "    Asserts 201 status and verifies response data.\n",
      "    \"\"\"\n",
      "    payload = get_file_attachment_payload()\n",
      "    response = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert response.status_code == 201, response.text\n",
      "    data = response.json()\n",
      "\n",
      "    # Check that response fields match input and system-generated fields exist\n",
      "    assert data[\"id\"] > 0\n",
      "    assert data[\"conversation_id\"] == payload[\"conversation_id\"]\n",
      "    assert data[\"original_filename\"] == payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == payload[\"file_size\"]\n",
      "    assert data[\"storage_path\"] == payload[\"storage_path\"]\n",
      "    assert data[\"extracted_text\"] == payload[\"extracted_text\"]\n",
      "    assert data[\"status\"] == payload[\"status\"]\n",
      "    assert data[\"additional_metadata\"] == payload[\"additional_metadata\"]\n",
      "    assert \"created_at\" in data\n",
      "    assert \"updated_at\" in data\n",
      "    # Check date format\n",
      "    datetime.fromisoformat(data[\"created_at\"])\n",
      "    datetime.fromisoformat(data[\"updated_at\"])\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: GET /file-attachments/\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_list_file_attachments():\n",
      "    \"\"\"\n",
      "    Test successful retrieval of all file attachments.\n",
      "    Asserts 200 status and verifies response structure.\n",
      "    \"\"\"\n",
      "    # Create at least one file attachment to ensure results\n",
      "    payload = get_file_attachment_payload(original_filename=\"overview.md\", file_type=\"md\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    created_id = create_resp.json()[\"id\"]\n",
      "\n",
      "    response = client.get(\"/file-attachments/\")\n",
      "    assert response.status_code == 200, response.text\n",
      "    attachments = response.json()\n",
      "    assert isinstance(attachments, list)\n",
      "    # At least one item should match the one we just created\n",
      "    found = False\n",
      "    for item in attachments:\n",
      "        assert \"id\" in item\n",
      "        assert \"conversation_id\" in item\n",
      "        assert \"original_filename\" in item\n",
      "        assert \"file_type\" in item\n",
      "        assert \"file_size\" in item\n",
      "        assert \"storage_path\" in item\n",
      "        assert \"extracted_text\" in item\n",
      "        assert \"status\" in item\n",
      "        assert \"additional_metadata\" in item\n",
      "        assert \"created_at\" in item\n",
      "        assert \"updated_at\" in item\n",
      "        if item[\"id\"] == created_id:\n",
      "            found = True\n",
      "            assert item[\"original_filename\"] == payload[\"original_filename\"]\n",
      "            assert item[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert found, \"Created item not found in GET /file-attachments/ list\"\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: GET /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_get_file_attachment_by_id():\n",
      "    \"\"\"\n",
      "    Test successful retrieval of a specific file attachment.\n",
      "    Asserts 200 status and verifies response data.\n",
      "    \"\"\"\n",
      "    payload = get_file_attachment_payload(original_filename=\"single.txt\", file_type=\"txt\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    created = create_resp.json()\n",
      "    file_id = created[\"id\"]\n",
      "\n",
      "    response = client.get(f\"/file-attachments/{file_id}\")\n",
      "    assert response.status_code == 200, response.text\n",
      "    data = response.json()\n",
      "    assert data[\"id\"] == file_id\n",
      "    assert data[\"original_filename\"] == payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == payload[\"file_size\"]\n",
      "    assert data[\"conversation_id\"] == payload[\"conversation_id\"]\n",
      "    assert \"created_at\" in data\n",
      "    assert \"updated_at\" in data\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: PUT /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_update_file_attachment_put():\n",
      "    \"\"\"\n",
      "    Test successful full update of a file attachment (PUT).\n",
      "    Asserts 200 status and verifies updated data.\n",
      "    \"\"\"\n",
      "    # Create original record\n",
      "    payload = get_file_attachment_payload(original_filename=\"put_test.md\", file_type=\"md\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    file_id = create_resp.json()[\"id\"]\n",
      "\n",
      "    # Prepare new data for full update\n",
      "    updated_payload = get_file_attachment_payload(\n",
      "        conversation_id=2,\n",
      "        original_filename=\"requirements_v2.docx\",\n",
      "        file_type=\"docx\",\n",
      "        file_size=54321,\n",
      "        storage_path=\"/storage/reqs/requirements_v2.docx\",\n",
      "        extracted_text=\"Updated extracted text.\",\n",
      "        status=\"processed\",\n",
      "        additional_metadata='{\"author\": \"Bob\"}',\n",
      "    )\n",
      "    resp = client.put(f\"/file-attachments/{file_id}\", json=updated_payload)\n",
      "    assert resp.status_code == 200, resp.text\n",
      "    data = resp.json()\n",
      "    assert data[\"id\"] == file_id\n",
      "    assert data[\"conversation_id\"] == updated_payload[\"conversation_id\"]\n",
      "    assert data[\"original_filename\"] == updated_payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == updated_payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == updated_payload[\"file_size\"]\n",
      "    assert data[\"storage_path\"] == updated_payload[\"storage_path\"]\n",
      "    assert data[\"extracted_text\"] == updated_payload[\"extracted_text\"]\n",
      "    assert data[\"status\"] == updated_payload[\"status\"]\n",
      "    assert data[\"additional_metadata\"] == updated_payload[\"additional_metadata\"]\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: PATCH /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_partial_update_file_attachment_patch():\n",
      "    \"\"\"\n",
      "    Test successful partial update of a file attachment (PATCH).\n",
      "    Asserts 200 status and verifies only updated fields change.\n",
      "    \"\"\"\n",
      "    # Create original\n",
      "    payload = get_file_attachment_payload(original_filename=\"patch_test.txt\", file_type=\"txt\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    orig = create_resp.json()\n",
      "    file_id = orig[\"id\"]\n",
      "\n",
      "    # Only update status and additional_metadata\n",
      "    patch_data = {\n",
      "        \"status\": \"processing\",\n",
      "        \"additional_metadata\": '{\"reviewed\": true}'\n",
      "    }\n",
      "    resp = client.patch(f\"/file-attachments/{file_id}\", json=patch_data)\n",
      "    assert resp.status_code == 200, resp.text\n",
      "    updated = resp.json()\n",
      "    # Unchanged fields\n",
      "    assert updated[\"conversation_id\"] == orig[\"conversation_id\"]\n",
      "    assert updated[\"original_filename\"] == orig[\"original_filename\"]\n",
      "    assert updated[\"file_type\"] == orig[\"file_type\"]\n",
      "    assert updated[\"file_size\"] == orig[\"file_size\"]\n",
      "    assert updated[\"storage_path\"] == orig[\"storage_path\"]\n",
      "    assert updated[\"extracted_text\"] == orig[\"extracted_text\"]\n",
      "    # Changed fields\n",
      "    assert updated[\"status\"] == patch_data[\"status\"]\n",
      "    assert updated[\"additional_metadata\"] == patch_data[\"additional_metadata\"]\n",
      "    assert updated[\"id\"] == file_id\n",
      "    assert \"created_at\" in updated\n",
      "    assert \"updated_at\" in updated\n",
      "\"\"\"\n",
      "Pytest Test Suite for File Attachment CRUD Endpoints\n",
      "\n",
      "This suite contains happy path tests for the main CRUD operations\n",
      "of the AI-Powered Requirement Analyzer's file attachment API.\n",
      "\n",
      "Requirements:\n",
      "- Tests are independent and self-contained.\n",
      "- Each test uses realistic data and validates status codes & response content.\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from datetime import datetime\n",
      "\n",
      "from main import app\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Helper: Default payload for file attachments\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def get_file_attachment_payload(\n",
      "    conversation_id=1,\n",
      "    original_filename=\"requirements.docx\",\n",
      "    file_type=\"docx\",\n",
      "    file_size=12345,\n",
      "    storage_path=\"/storage/reqs/requirements.docx\",\n",
      "    extracted_text=\"Extracted requirement text.\",\n",
      "    status=\"uploaded\",\n",
      "    additional_metadata='{\"author\": \"Alice\"}'\n",
      "):\n",
      "    return {\n",
      "        \"conversation_id\": conversation_id,\n",
      "        \"original_filename\": original_filename,\n",
      "        \"file_type\": file_type,\n",
      "        \"file_size\": file_size,\n",
      "        \"storage_path\": storage_path,\n",
      "        \"extracted_text\": extracted_text,\n",
      "        \"status\": status,\n",
      "        \"additional_metadata\": additional_metadata,\n",
      "    }\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: POST /file-attachments/\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_create_file_attachment():\n",
      "    \"\"\"\n",
      "    Test successful creation of a file attachment.\n",
      "    Asserts 201 status and verifies response data.\n",
      "    \"\"\"\n",
      "    payload = get_file_attachment_payload()\n",
      "    response = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert response.status_code == 201, response.text\n",
      "    data = response.json()\n",
      "\n",
      "    # Check that response fields match input and system-generated fields exist\n",
      "    assert data[\"id\"] > 0\n",
      "    assert data[\"conversation_id\"] == payload[\"conversation_id\"]\n",
      "    assert data[\"original_filename\"] == payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == payload[\"file_size\"]\n",
      "    assert data[\"storage_path\"] == payload[\"storage_path\"]\n",
      "    assert data[\"extracted_text\"] == payload[\"extracted_text\"]\n",
      "    assert data[\"status\"] == payload[\"status\"]\n",
      "    assert data[\"additional_metadata\"] == payload[\"additional_metadata\"]\n",
      "    assert \"created_at\" in data\n",
      "    assert \"updated_at\" in data\n",
      "    # Check date format\n",
      "    datetime.fromisoformat(data[\"created_at\"])\n",
      "    datetime.fromisoformat(data[\"updated_at\"])\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: GET /file-attachments/\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_list_file_attachments():\n",
      "    \"\"\"\n",
      "    Test successful retrieval of all file attachments.\n",
      "    Asserts 200 status and verifies response structure.\n",
      "    \"\"\"\n",
      "    # Create at least one file attachment to ensure results\n",
      "    payload = get_file_attachment_payload(original_filename=\"overview.md\", file_type=\"md\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    created_id = create_resp.json()[\"id\"]\n",
      "\n",
      "    response = client.get(\"/file-attachments/\")\n",
      "    assert response.status_code == 200, response.text\n",
      "    attachments = response.json()\n",
      "    assert isinstance(attachments, list)\n",
      "    # At least one item should match the one we just created\n",
      "    found = False\n",
      "    for item in attachments:\n",
      "        assert \"id\" in item\n",
      "        assert \"conversation_id\" in item\n",
      "        assert \"original_filename\" in item\n",
      "        assert \"file_type\" in item\n",
      "        assert \"file_size\" in item\n",
      "        assert \"storage_path\" in item\n",
      "        assert \"extracted_text\" in item\n",
      "        assert \"status\" in item\n",
      "        assert \"additional_metadata\" in item\n",
      "        assert \"created_at\" in item\n",
      "        assert \"updated_at\" in item\n",
      "        if item[\"id\"] == created_id:\n",
      "            found = True\n",
      "            assert item[\"original_filename\"] == payload[\"original_filename\"]\n",
      "            assert item[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert found, \"Created item not found in GET /file-attachments/ list\"\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: GET /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_get_file_attachment_by_id():\n",
      "    \"\"\"\n",
      "    Test successful retrieval of a specific file attachment.\n",
      "    Asserts 200 status and verifies response data.\n",
      "    \"\"\"\n",
      "    payload = get_file_attachment_payload(original_filename=\"single.txt\", file_type=\"txt\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    created = create_resp.json()\n",
      "    file_id = created[\"id\"]\n",
      "\n",
      "    response = client.get(f\"/file-attachments/{file_id}\")\n",
      "    assert response.status_code == 200, response.text\n",
      "    data = response.json()\n",
      "    assert data[\"id\"] == file_id\n",
      "    assert data[\"original_filename\"] == payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == payload[\"file_size\"]\n",
      "    assert data[\"conversation_id\"] == payload[\"conversation_id\"]\n",
      "    assert \"created_at\" in data\n",
      "    assert \"updated_at\" in data\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: PUT /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_update_file_attachment_put():\n",
      "    \"\"\"\n",
      "    Test successful full update of a file attachment (PUT).\n",
      "    Asserts 200 status and verifies updated data.\n",
      "    \"\"\"\n",
      "    # Create original record\n",
      "    payload = get_file_attachment_payload(original_filename=\"put_test.md\", file_type=\"md\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    file_id = create_resp.json()[\"id\"]\n",
      "\n",
      "    # Prepare new data for full update\n",
      "    updated_payload = get_file_attachment_payload(\n",
      "        conversation_id=2,\n",
      "        original_filename=\"requirements_v2.docx\",\n",
      "        file_type=\"docx\",\n",
      "        file_size=54321,\n",
      "        storage_path=\"/storage/reqs/requirements_v2.docx\",\n",
      "        extracted_text=\"Updated extracted text.\",\n",
      "        status=\"processed\",\n",
      "        additional_metadata='{\"author\": \"Bob\"}',\n",
      "    )\n",
      "    resp = client.put(f\"/file-attachments/{file_id}\", json=updated_payload)\n",
      "    assert resp.status_code == 200, resp.text\n",
      "    data = resp.json()\n",
      "    assert data[\"id\"] == file_id\n",
      "    assert data[\"conversation_id\"] == updated_payload[\"conversation_id\"]\n",
      "    assert data[\"original_filename\"] == updated_payload[\"original_filename\"]\n",
      "    assert data[\"file_type\"] == updated_payload[\"file_type\"]\n",
      "    assert data[\"file_size\"] == updated_payload[\"file_size\"]\n",
      "    assert data[\"storage_path\"] == updated_payload[\"storage_path\"]\n",
      "    assert data[\"extracted_text\"] == updated_payload[\"extracted_text\"]\n",
      "    assert data[\"status\"] == updated_payload[\"status\"]\n",
      "    assert data[\"additional_metadata\"] == updated_payload[\"additional_metadata\"]\n",
      "\n",
      "# --------------------------------------------------------------------------\n",
      "# Test: PATCH /file-attachments/{id}\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "def test_partial_update_file_attachment_patch():\n",
      "    \"\"\"\n",
      "    Test successful partial update of a file attachment (PATCH).\n",
      "    Asserts 200 status and verifies only updated fields change.\n",
      "    \"\"\"\n",
      "    # Create original\n",
      "    payload = get_file_attachment_payload(original_filename=\"patch_test.txt\", file_type=\"txt\")\n",
      "    create_resp = client.post(\"/file-attachments/\", json=payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    orig = create_resp.json()\n",
      "    file_id = orig[\"id\"]\n",
      "\n",
      "    # Only update status and additional_metadata\n",
      "    patch_data = {\n",
      "        \"status\": \"processing\",\n",
      "        \"additional_metadata\": '{\"reviewed\": true}'\n",
      "    }\n",
      "    resp = client.patch(f\"/file-attachments/{file_id}\", json=patch_data)\n",
      "    assert resp.status_code == 200, resp.text\n",
      "    updated = resp.json()\n",
      "    # Unchanged fields\n",
      "    assert updated[\"conversation_id\"] == orig[\"conversation_id\"]\n",
      "    assert updated[\"original_filename\"] == orig[\"original_filename\"]\n",
      "    assert updated[\"file_type\"] == orig[\"file_type\"]\n",
      "    assert updated[\"file_size\"] == orig[\"file_size\"]\n",
      "    assert updated[\"storage_path\"] == orig[\"storage_path\"]\n",
      "    assert updated[\"extracted_text\"] == orig[\"extracted_text\"]\n",
      "    # Changed fields\n",
      "    assert updated[\"status\"] == patch_data[\"status\"]\n",
      "    assert updated[\"additional_metadata\"] == patch_data[\"additional_metadata\"]\n",
      "    assert updated[\"id\"] == file_id\n",
      "    assert \"created_at\" in updated\n",
      "    assert \"updated_at\" in updated\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate happy path tests for your API.\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer tasked with creating comprehensive pytest tests for a FastAPI application. \n",
    "\n",
    "Please analyze the following FastAPI application code and generate happy path tests that verify the successful operation of the main CRUD endpoints:\n",
    "\n",
    "APPLICATION CODE:\n",
    "{app_code}\n",
    "\n",
    "Requirements for the tests:\n",
    "1. Act as a professional QA Engineer\n",
    "2. Generate pytest test functions for the following happy path scenarios:\n",
    "   - POST /file-attachments/ - Test successful creation of a file attachment (assert 201 status and verify response data)\n",
    "   - GET /file-attachments/ - Test successful retrieval of all file attachments (assert 200 status and verify response structure)\n",
    "   - GET /file-attachments/{{id}} - Test successful retrieval of a specific file attachment (assert 200 status and verify response data)\n",
    "   - PUT /file-attachments/{{id}} - Test successful full update of a file attachment (assert 200 status and verify updated data)\n",
    "   - PATCH /file-attachments/{{id}} - Test successful partial update of a file attachment (assert 200 status and verify updated fields)\n",
    "\n",
    "3. Include necessary imports (pytest, FastAPI TestClient, etc.)\n",
    "4. Use FastAPI's TestClient for testing\n",
    "5. Create realistic test data that matches the Pydantic models\n",
    "6. Include proper assertions for status codes and response content\n",
    "7. Follow pytest naming conventions (test_*)\n",
    "8. Add descriptive docstrings for each test function\n",
    "9. Make tests independent and self-contained\n",
    "\n",
    "Generate clean, production-ready pytest code that thoroughly tests the happy path scenarios for this file attachment API.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_happy_paths.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate edge case tests.\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer tasked with creating comprehensive edge case tests for a FastAPI application. \n",
    "\n",
    "Please analyze the following FastAPI application code and generate edge case tests that verify the application handles common error scenarios correctly:\n",
    "\n",
    "APPLICATION CODE:\n",
    "{app_code}\n",
    "\n",
    "Requirements for the edge case tests:\n",
    "1. Act as a professional QA Engineer focused on breaking the application\n",
    "2. Generate 5-8 pytest test functions covering the following edge case scenarios:\n",
    "   \n",
    "   **Invalid Data Scenarios:**\n",
    "   - POST /file-attachments/ with invalid file_size (negative, zero, exceeding 50MB limit)\n",
    "   - POST /file-attachments/ with invalid conversation_id (negative, zero)\n",
    "   - POST /file-attachments/ with invalid file_type (unsupported format)\n",
    "   - POST /file-attachments/ with missing required fields\n",
    "   - POST /file-attachments/ with empty/invalid original_filename\n",
    "   \n",
    "   **Non-existent Resource Scenarios:**\n",
    "   - GET /file-attachments/{{non_existent_id}} (assert 404 Not Found)\n",
    "   - PUT /file-attachments/{{non_existent_id}} (assert 404 Not Found)\n",
    "   - PATCH /file-attachments/{{non_existent_id}} (assert 404 Not Found)\n",
    "   - DELETE /file-attachments/{{non_existent_id}} (assert 404 Not Found)\n",
    "   \n",
    "   **Invalid ID Scenarios:**\n",
    "   - GET/PUT/PATCH/DELETE with negative file_id (assert 400 Bad Request)\n",
    "   - GET/PUT/PATCH/DELETE with zero file_id (assert 400 Bad Request)\n",
    "   \n",
    "   **Empty Update Scenarios:**\n",
    "   - PATCH /file-attachments/{{id}} with empty request body (assert 400 Bad Request)\n",
    "\n",
    "3. Include necessary imports (pytest, FastAPI TestClient, etc.)\n",
    "4. Use FastAPI's TestClient for testing\n",
    "5. Include proper assertions for status codes and error messages\n",
    "6. Follow pytest naming conventions (test_*)\n",
    "7. Add descriptive docstrings for each test function explaining the edge case\n",
    "8. Make tests independent and self-contained\n",
    "9. Test both validation errors (422) and business logic errors (400/404)\n",
    "10. Verify error response format and meaningful error messages\n",
    "\n",
    "Generate clean, production-ready pytest code that thoroughly tests edge cases and error handling for this file attachment API. Focus on scenarios that could break the application or cause security issues.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "    save_artifact(cleaned_edge_case_tests, \"tests/test_edge_cases.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the pytest fixture for an isolated test database.\n",
    "db_fixture_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# TODO: Write a prompt to refactor the happy path tests to use the new fixture.\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
